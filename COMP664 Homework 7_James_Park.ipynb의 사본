{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1y3j4FK37bsw5XbydT462FLBPyPS1i2Fd","timestamp":1678257967399},{"file_id":"1g-Yt2Cj6PFLYY4jylSgPsRMp-150uUQB","timestamp":1678066621459}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"t2hPDx3Tvh0v"},"source":["# Homework 7\n","\n","In this homework you will be training and using a \"char-RNN\". This is the name given to a character-level recurrent neural network language model by [this famous blog post by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Before you start on the rest of the homework, please give the blog post a read, it's quite good!\n","\n","I don't expect you to implement the char-RNN from scratch. Andrej's original char-rnn is in Torch (the predecessor to PyTorch that is not commonly used anymore). Fortunately, there are many other implementations of this model available; for example, there is one (in both mxnet and pytorch) in chapters 8 and 9 of [the textbook](http://d2l.ai), and another pytorch one [here](https://github.com/spro/char-rnn.pytorch). **Please use one of these example implementations (or another one that you find) when completing this homework**.\n","\n","For this homework, please complete the following steps:\n","\n","1. Download and tokenize the [Shakespeare dataset](http://www.gutenberg.org/files/100/100-0.txt) at a character level. I recommend basing your solution on the following code:\n","```Python\n","# Remove non-alphabetical characters, lowercase, and replace whitespace with ' '\n","raw_dataset = ' '.join(re.sub('[^A-Za-z ]+', '', text).lower().split())\n","# Maps token index to character\n","idx_to_char = list(set(raw_dataset))\n","# Maps character to token index\n","char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n","# Tokenize the dataset\n","corpus_indices = [char_to_idx[char] for char in raw_dataset]\n","```\n","1. Train a \"vanilla\" RNN (as described in chapter 9 of [the textbook](http://d2l.ai)) on the Shakespeare dataset. Report the training loss and generate some samples from the model at the end of training.\n","1. Train a GRU RNN (as described in chapter 10 of [the textbook](http://d2l.ai)) on the Shakespeare datatset. Is the final training loss higher or lower than the vanilla RNN? Are the samples from the model more or less realistic?\n","1. Find a smaller, simpler dataset than the Shakespeare data (you can find some ideas in Andrej's blog post, but feel free to get creative!) and train either the vanilla or GRU RNN on it instead. Is the final training loss higher or lower than it was for the Shakespeare data?"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import re\n","\n","torch.manual_seed(42)\n","\n","# data I/O\n","data = open('shakespeare.txt', 'r').read()\n","raw_dataset = ' '.join(re.sub('[^A-Za-z ]+', '', data).lower().split())\n","data_size = len(raw_dataset)\n","chars = list(set(raw_dataset))\n","vocab_size = len(chars)\n","char_to_ix = {ch: i for i, ch in enumerate(chars)}\n","ix_to_char = {i: ch for i, ch in enumerate(chars)}\n","\n","# hyperparameters\n","hidden_size = 100 \n","seq_length = 100\n","learning_rate = 1e-1\n","\n","# define the model\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = nn.RNN(hidden_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","        \n","    def forward(self, inputs, hidden):\n","      embeds = self.embedding(inputs).view(-1, 1, self.hidden_size)\n","    \n","      output, hidden = self.rnn(embeds, hidden)\n","    \n","      output = output.view(-1, self.hidden_size)\n","      output = self.fc(output)\n","    \n","      return output, hidden\n","\n","model = RNN(vocab_size, hidden_size)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    h = torch.zeros(1, 1, hidden_size)\n","    \n","    inputs = [char_to_ix[ch] for ch in raw_dataset[:seq_length]]\n","    targets = [char_to_ix[ch] for ch in raw_dataset[1:seq_length+1]]\n","    inputs = torch.tensor(inputs, dtype=torch.long)\n","    targets = torch.tensor(targets, dtype=torch.long)\n","    \n","    loss = 0\n","    for t in range(seq_length):\n","        output, h = model(inputs[t], h)\n","        loss += criterion(output, targets[t].unsqueeze(0))\n","        \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    if (epoch+1) % 10 == 0:\n","        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n","\n","def generate_text(model, start_sentence, num_chars):\n","    model.eval()\n","\n","    train_chars = set(char_to_ix.keys())\n","    for c in start_sentence:\n","        if c not in train_chars:\n","            raise ValueError(\"Starting words contains character '{}' not present in training dataset\".format(c))\n","\n","    h = torch.zeros(1, 1, hidden_size)\n","    input = torch.tensor([char_to_ix[start_sentence[0]]], dtype=torch.long)\n","\n","    text = start_sentence\n","    for i in range(num_chars):\n","        output, h = model(input, h)\n","\n","        probs = nn.functional.softmax(output, dim=1).detach().numpy().squeeze()\n","        next_char_ix = np.random.choice(vocab_size, p=probs)\n","        next_char = ix_to_char[next_char_ix]\n","\n","        text += next_char\n","\n","        input = torch.tensor([next_char_ix], dtype=torch.long)\n","\n","    return text\n","\n","generated_text = generate_text(model, \"p\", 1000)\n","print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sN4XpUz8n7fl","executionInfo":{"status":"ok","timestamp":1678257081819,"user_tz":300,"elapsed":44715,"user":{"displayName":"Jeong Ho Park","userId":"12726191324453319419"}},"outputId":"bdc20688-44f5-4a3d-e349-6d65bdd02bec"},"execution_count":171,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/1000], Loss: 42.8321\n","Epoch [20/1000], Loss: 7.0544\n","Epoch [30/1000], Loss: 5.1331\n","Epoch [40/1000], Loss: 4.5001\n","Epoch [50/1000], Loss: 3.4457\n","Epoch [60/1000], Loss: 4.3234\n","Epoch [70/1000], Loss: 5.5800\n","Epoch [80/1000], Loss: 4.4517\n","Epoch [90/1000], Loss: 4.2918\n","Epoch [100/1000], Loss: 4.2158\n","Epoch [110/1000], Loss: 4.1915\n","Epoch [120/1000], Loss: 4.1868\n","Epoch [130/1000], Loss: 3.9550\n","Epoch [140/1000], Loss: 4.1993\n","Epoch [150/1000], Loss: 4.5838\n","Epoch [160/1000], Loss: 4.3677\n","Epoch [170/1000], Loss: 4.2113\n","Epoch [180/1000], Loss: 4.1930\n","Epoch [190/1000], Loss: 4.2220\n","Epoch [200/1000], Loss: 4.2253\n","Epoch [210/1000], Loss: 4.2007\n","Epoch [220/1000], Loss: 4.1858\n","Epoch [230/1000], Loss: 4.2268\n","Epoch [240/1000], Loss: 4.1774\n","Epoch [250/1000], Loss: 4.1821\n","Epoch [260/1000], Loss: 4.1727\n","Epoch [270/1000], Loss: 4.1723\n","Epoch [280/1000], Loss: 4.1835\n","Epoch [290/1000], Loss: 4.2623\n","Epoch [300/1000], Loss: 4.2520\n","Epoch [310/1000], Loss: 4.1721\n","Epoch [320/1000], Loss: 4.1787\n","Epoch [330/1000], Loss: 4.1736\n","Epoch [340/1000], Loss: 4.1696\n","Epoch [350/1000], Loss: 4.1683\n","Epoch [360/1000], Loss: 4.1681\n","Epoch [370/1000], Loss: 4.1674\n","Epoch [380/1000], Loss: 4.1671\n","Epoch [390/1000], Loss: 4.1668\n","Epoch [400/1000], Loss: 4.1665\n","Epoch [410/1000], Loss: 4.1663\n","Epoch [420/1000], Loss: 4.1660\n","Epoch [430/1000], Loss: 4.1657\n","Epoch [440/1000], Loss: 4.1654\n","Epoch [450/1000], Loss: 4.1652\n","Epoch [460/1000], Loss: 4.1649\n","Epoch [470/1000], Loss: 4.1647\n","Epoch [480/1000], Loss: 4.1640\n","Epoch [490/1000], Loss: 4.2374\n","Epoch [500/1000], Loss: 4.3488\n","Epoch [510/1000], Loss: 7.2511\n","Epoch [520/1000], Loss: 122.0981\n","Epoch [530/1000], Loss: 346.2987\n","Epoch [540/1000], Loss: 291.3483\n","Epoch [550/1000], Loss: 192.0668\n","Epoch [560/1000], Loss: 233.8404\n","Epoch [570/1000], Loss: 211.0756\n","Epoch [580/1000], Loss: 134.7264\n","Epoch [590/1000], Loss: 103.5127\n","Epoch [600/1000], Loss: 70.9034\n","Epoch [610/1000], Loss: 58.8792\n","Epoch [620/1000], Loss: 46.1105\n","Epoch [630/1000], Loss: 33.6392\n","Epoch [640/1000], Loss: 48.9249\n","Epoch [650/1000], Loss: 44.8382\n","Epoch [660/1000], Loss: 44.5773\n","Epoch [670/1000], Loss: 34.5180\n","Epoch [680/1000], Loss: 22.9364\n","Epoch [690/1000], Loss: 21.5157\n","Epoch [700/1000], Loss: 19.9744\n","Epoch [710/1000], Loss: 18.3092\n","Epoch [720/1000], Loss: 18.7229\n","Epoch [730/1000], Loss: 17.8282\n","Epoch [740/1000], Loss: 17.7973\n","Epoch [750/1000], Loss: 16.9015\n","Epoch [760/1000], Loss: 16.6265\n","Epoch [770/1000], Loss: 16.8136\n","Epoch [780/1000], Loss: 16.6149\n","Epoch [790/1000], Loss: 16.5222\n","Epoch [800/1000], Loss: 16.7460\n","Epoch [810/1000], Loss: 16.5785\n","Epoch [820/1000], Loss: 16.4988\n","Epoch [830/1000], Loss: 16.7336\n","Epoch [840/1000], Loss: 16.5639\n","Epoch [850/1000], Loss: 16.4851\n","Epoch [860/1000], Loss: 16.7195\n","Epoch [870/1000], Loss: 16.5536\n","Epoch [880/1000], Loss: 16.4753\n","Epoch [890/1000], Loss: 16.7096\n","Epoch [900/1000], Loss: 16.5441\n","Epoch [910/1000], Loss: 16.4665\n","Epoch [920/1000], Loss: 16.7020\n","Epoch [930/1000], Loss: 16.5364\n","Epoch [940/1000], Loss: 16.4591\n","Epoch [950/1000], Loss: 16.6951\n","Epoch [960/1000], Loss: 16.5299\n","Epoch [970/1000], Loss: 16.4529\n","Epoch [980/1000], Loss: 16.6890\n","Epoch [990/1000], Loss: 16.5242\n","Epoch [1000/1000], Loss: 16.4474\n","plete project gutenberg ebook oject gute project gutenberg ebook oject gutenbe by works ebook of the completenberg ebook oject gutenberg ebook oject gutenberg ebook oject gutenberg ebook of the prof william shakespearethis ebook oject gute william shakespearete completenberg ebook of the works oject gute prof william shakespearetenberg ebook oject gutenberg ebook oject gutenberg ebook oject gutenberg ebook of the complethis of william shakespeare by william shakespeare works oject gute complethis ebook of works of the works ebook oject gutenberg ebook oject gutenberg ebook of works ebook of the william shakespearete complethis ebook oject gutenberg ebook oject gute works ebook of william shakespearethis ebook of the complete william shakespearetenberg ebook of works ebook oject gutenberg ebook of the project gute complethis oject gutenberg ebook of the works of william shakespearetenberg ebook oject gutenberg ebook of william shakespearetenberg ebook oject gute by william shakespeare by\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","torch.manual_seed(42)\n","\n","# data I/O\n","data = open('shakespeare.txt', 'r').read() \n","raw_dataset = ' '.join(re.sub('[^A-Za-z ]+', '', data).lower().split())\n","data_size = len(raw_dataset)\n","chars = list(set(raw_dataset))\n","vocab_size = len(chars)\n","char_to_ix = { ch:i for i,ch in enumerate(chars) }\n","ix_to_char = { i:ch for i,ch in enumerate(chars) }\n","\n","# hyperparameters\n","hidden_size = 100 \n","seq_length = 100\n","learning_rate = 1e-1\n","num_layers = 1 \n","\n","# define the model\n","class GRU(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers):\n","        super(GRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","        \n","    def forward(self, inputs, hidden):\n","        embeds = self.embedding(inputs).view(-1, 1, self.hidden_size)\n","        \n","        output, hidden = self.gru(embeds, hidden)\n","        \n","        output = output.view(-1, self.hidden_size)\n","        output = self.fc(output)\n","        \n","        return output, hidden\n","\n","model = GRU(vocab_size, hidden_size, num_layers).cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    h = torch.zeros(num_layers, 1, hidden_size).cuda()\n","    \n","    inputs = [char_to_ix[ch] for ch in raw_dataset[:seq_length]]\n","    targets = [char_to_ix[ch] for ch in raw_dataset[1:seq_length+1]]\n","    inputs = torch.tensor(inputs, dtype=torch.long).cuda()\n","    targets = torch.tensor(targets, dtype=torch.long).cuda()\n","    \n","    loss = 0\n","    for t in range(seq_length):\n","        output, h = model(inputs[t], h)\n","        loss += criterion(output, targets[t].unsqueeze(0))\n","        \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    if (epoch+1) % 10 == 0:\n","        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n","\n","def generate_text(model, start_sentence, num_chars):\n","    \n","    model.eval()\n","\n","    train_chars = set(char_to_ix.keys())\n","    for c in start_sentence:\n","        if c not in train_chars:\n","            raise ValueError(\"Starting word contains character '{}' not present in training dataset\".format(c))\n","\n","    h = torch.zeros(num_layers, 1, hidden_size).cuda()\n","    input = torch.tensor([char_to_ix[start_sentence[0]]], dtype=torch.long).cuda()\n","\n","    text = start_sentence\n","    for i in range(num_chars):\n","        output, h = model(input, h)\n","\n","        probs = nn.functional.softmax(output, dim=1).detach().cpu().numpy().squeeze()\n","        next_char_ix = np.random.choice(vocab_size, p=probs)\n","        next_char = ix_to_char[next_char_ix]\n","\n","        text += next_char\n","\n","        input = torch.tensor([next_char_ix], dtype=torch.long).cuda()\n","\n","    return text\n","\n","\n","generated_text = generate_text(model, \"p\", 1000)\n","print(generated_text)"],"metadata":{"id":"gxiKT8qmstNe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678257642097,"user_tz":300,"elapsed":77108,"user":{"displayName":"Jeong Ho Park","userId":"12726191324453319419"}},"outputId":"102fcb4b-e1eb-4274-9333-9e4137163b39"},"execution_count":174,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/1000], Loss: 5.5449\n","Epoch [20/1000], Loss: 0.7423\n","Epoch [30/1000], Loss: 0.1817\n","Epoch [40/1000], Loss: 0.0623\n","Epoch [50/1000], Loss: 0.0307\n","Epoch [60/1000], Loss: 0.0220\n","Epoch [70/1000], Loss: 0.0181\n","Epoch [80/1000], Loss: 0.0158\n","Epoch [90/1000], Loss: 0.0143\n","Epoch [100/1000], Loss: 0.0131\n","Epoch [110/1000], Loss: 0.0121\n","Epoch [120/1000], Loss: 0.0111\n","Epoch [130/1000], Loss: 0.0103\n","Epoch [140/1000], Loss: 0.0097\n","Epoch [150/1000], Loss: 0.0091\n","Epoch [160/1000], Loss: 0.0086\n","Epoch [170/1000], Loss: 0.0081\n","Epoch [180/1000], Loss: 0.0077\n","Epoch [190/1000], Loss: 0.0073\n","Epoch [200/1000], Loss: 0.0070\n","Epoch [210/1000], Loss: 0.0067\n","Epoch [220/1000], Loss: 0.0064\n","Epoch [230/1000], Loss: 0.0061\n","Epoch [240/1000], Loss: 0.0058\n","Epoch [250/1000], Loss: 0.0056\n","Epoch [260/1000], Loss: 0.0054\n","Epoch [270/1000], Loss: 0.0052\n","Epoch [280/1000], Loss: 0.0050\n","Epoch [290/1000], Loss: 0.0048\n","Epoch [300/1000], Loss: 0.0046\n","Epoch [310/1000], Loss: 0.0045\n","Epoch [320/1000], Loss: 0.0043\n","Epoch [330/1000], Loss: 0.0042\n","Epoch [340/1000], Loss: 0.0040\n","Epoch [350/1000], Loss: 0.0039\n","Epoch [360/1000], Loss: 0.0038\n","Epoch [370/1000], Loss: 0.0036\n","Epoch [380/1000], Loss: 0.0035\n","Epoch [390/1000], Loss: 0.0034\n","Epoch [400/1000], Loss: 0.0033\n","Epoch [410/1000], Loss: 0.0032\n","Epoch [420/1000], Loss: 0.0031\n","Epoch [430/1000], Loss: 0.0030\n","Epoch [440/1000], Loss: 0.0029\n","Epoch [450/1000], Loss: 0.0028\n","Epoch [460/1000], Loss: 0.0028\n","Epoch [470/1000], Loss: 0.0027\n","Epoch [480/1000], Loss: 0.0026\n","Epoch [490/1000], Loss: 0.0026\n","Epoch [500/1000], Loss: 0.0025\n","Epoch [510/1000], Loss: 0.0024\n","Epoch [520/1000], Loss: 0.0024\n","Epoch [530/1000], Loss: 0.0023\n","Epoch [540/1000], Loss: 0.0023\n","Epoch [550/1000], Loss: 0.0022\n","Epoch [560/1000], Loss: 0.0022\n","Epoch [570/1000], Loss: 0.0021\n","Epoch [580/1000], Loss: 0.0021\n","Epoch [590/1000], Loss: 0.0020\n","Epoch [600/1000], Loss: 0.0020\n","Epoch [610/1000], Loss: 0.0020\n","Epoch [620/1000], Loss: 0.0019\n","Epoch [630/1000], Loss: 0.0019\n","Epoch [640/1000], Loss: 0.0018\n","Epoch [650/1000], Loss: 0.0018\n","Epoch [660/1000], Loss: 0.0018\n","Epoch [670/1000], Loss: 0.0017\n","Epoch [680/1000], Loss: 0.0017\n","Epoch [690/1000], Loss: 0.0017\n","Epoch [700/1000], Loss: 0.0016\n","Epoch [710/1000], Loss: 0.0016\n","Epoch [720/1000], Loss: 0.0016\n","Epoch [730/1000], Loss: 0.0015\n","Epoch [740/1000], Loss: 0.0015\n","Epoch [750/1000], Loss: 0.0015\n","Epoch [760/1000], Loss: 0.0015\n","Epoch [770/1000], Loss: 0.0014\n","Epoch [780/1000], Loss: 0.0014\n","Epoch [790/1000], Loss: 0.0014\n","Epoch [800/1000], Loss: 0.0014\n","Epoch [810/1000], Loss: 0.0014\n","Epoch [820/1000], Loss: 0.0013\n","Epoch [830/1000], Loss: 0.0013\n","Epoch [840/1000], Loss: 0.0013\n","Epoch [850/1000], Loss: 0.0013\n","Epoch [860/1000], Loss: 0.0013\n","Epoch [870/1000], Loss: 0.0012\n","Epoch [880/1000], Loss: 0.0012\n","Epoch [890/1000], Loss: 0.0012\n","Epoch [900/1000], Loss: 0.0012\n","Epoch [910/1000], Loss: 0.0012\n","Epoch [920/1000], Loss: 0.0012\n","Epoch [930/1000], Loss: 0.0011\n","Epoch [940/1000], Loss: 0.0011\n","Epoch [950/1000], Loss: 0.0011\n","Epoch [960/1000], Loss: 0.0011\n","Epoch [970/1000], Loss: 0.0011\n","Epoch [980/1000], Loss: 0.0011\n","Epoch [990/1000], Loss: 0.0011\n","Epoch [1000/1000], Loss: 0.0010\n","plete works of william shakespeare by william shakespearethis e project gutenberg ebook of the complete works of william shakespeare by william shakespearethis eorks of william shakespeare by william shakespearethis e project gutenberg ebook of the complete works of william shakespeare by william shakespearethis e complete works of william shakespeare by william shakespearethis e complete works of william shakespeare by william shakespearethis e complete works of william shakespeare by william shakespearethis es ecomplete works of william shakespeare by william shakespearethis ecomplete works of william shakespeare by william shakespearethis e complete works of william shakespeare by william shakespearethis e complete works of william shakespeare by william shakespearethis e project gutenberg ebook of the complete works of william shakespeare by william shakespearethis e complete works of william shakespeare by william shakespearethis e complete works of william shakespeare by william s\n"]}]},{"cell_type":"markdown","source":["It shows that the GRU RNN converges with the error of 0.001; on the other hand vanilla RNN has not shown that it converges with a low/stable loss value. \n","\n","The output of the vanilla RNN model shows that the model did not learn the sequencing of the words by producing randomly sequenced words. In contrast, it shows that the GRU RNN model had somewhat good sequencing of words produced with individual characters. "],"metadata":{"id":"lkg0kYCQaFyu"}},{"cell_type":"markdown","source":["\n","\n","> HARRY POTTER DATASET (First book)\n","\n"],"metadata":{"id":"AY8W7bo-cOTa"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import re\n","\n","# data I/O\n","data = open('harry_potter.txt', 'r').read()\n","raw_dataset = ' '.join(re.sub('[^A-Za-z ]+', '', data).lower().split())\n","data_size = len(raw_dataset)\n","chars = list(set(raw_dataset))\n","vocab_size = len(chars)\n","char_to_ix = {ch: i for i, ch in enumerate(chars)}\n","ix_to_char = {i: ch for i, ch in enumerate(chars)}\n","\n","# hyperparameters\n","hidden_size = 100 \n","seq_length = 50 \n","learning_rate = 1e-2\n","\n","# define the model\n","class RNN(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(RNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = nn.RNN(hidden_size, hidden_size)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","        \n","    def forward(self, inputs, hidden):\n","      embeds = self.embedding(inputs).view(-1, 1, self.hidden_size)\n","    \n","      output, hidden = self.rnn(embeds, hidden)\n","    \n","      output = output.view(-1, self.hidden_size)\n","      output = self.fc(output)\n","    \n","      return output, hidden\n","\n","model = RNN(vocab_size, hidden_size)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    h = torch.zeros(1, 1, hidden_size)\n","    \n","    inputs = [char_to_ix[ch] for ch in raw_dataset[:seq_length]]\n","    targets = [char_to_ix[ch] for ch in raw_dataset[1:seq_length+1]]\n","    inputs = torch.tensor(inputs, dtype=torch.long)\n","    targets = torch.tensor(targets, dtype=torch.long)\n","    \n","    loss = 0\n","    for t in range(seq_length):\n","        output, h = model(inputs[t], h)\n","        loss += criterion(output, targets[t].unsqueeze(0))\n","        \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    if (epoch+1) % 10 == 0:\n","        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n","\n","def generate_text(model, start_sentence, num_chars):\n","    model.eval()\n","\n","    train_chars = set(char_to_ix.keys())\n","    for c in start_sentence:\n","        if c not in train_chars:\n","            raise ValueError(\"Starting words contains character '{}' not present in training dataset\".format(c))\n","\n","    h = torch.zeros(1, 1, hidden_size)\n","    input = torch.tensor([char_to_ix[start_sentence[0]]], dtype=torch.long)\n","\n","    text = start_sentence\n","    for i in range(num_chars):\n","        output, h = model(input, h)\n","\n","        probs = nn.functional.softmax(output, dim=1).detach().numpy().squeeze()\n","        next_char_ix = np.random.choice(vocab_size, p=probs)\n","        next_char = ix_to_char[next_char_ix]\n","\n","        text += next_char\n","\n","        input = torch.tensor([next_char_ix], dtype=torch.long)\n","\n","    return text\n","\n","generated_text = generate_text(model, \"t\", 1000)\n","print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lYWvghmRHdAp","executionInfo":{"status":"ok","timestamp":1678253812831,"user_tz":300,"elapsed":21997,"user":{"displayName":"Jeong Ho Park","userId":"12726191324453319419"}},"outputId":"14eb2b64-df63-4817-fbd7-51d8b684d8d3"},"execution_count":142,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/1000], Loss: 7.5527\n","Epoch [20/1000], Loss: 0.4183\n","Epoch [30/1000], Loss: 0.1027\n","Epoch [40/1000], Loss: 0.0484\n","Epoch [50/1000], Loss: 0.0332\n","Epoch [60/1000], Loss: 0.0269\n","Epoch [70/1000], Loss: 0.0234\n","Epoch [80/1000], Loss: 0.0211\n","Epoch [90/1000], Loss: 0.0193\n","Epoch [100/1000], Loss: 0.0179\n","Epoch [110/1000], Loss: 0.0167\n","Epoch [120/1000], Loss: 0.0156\n","Epoch [130/1000], Loss: 0.0146\n","Epoch [140/1000], Loss: 0.0138\n","Epoch [150/1000], Loss: 0.0130\n","Epoch [160/1000], Loss: 0.0123\n","Epoch [170/1000], Loss: 0.0117\n","Epoch [180/1000], Loss: 0.0111\n","Epoch [190/1000], Loss: 0.0105\n","Epoch [200/1000], Loss: 0.0101\n","Epoch [210/1000], Loss: 0.0096\n","Epoch [220/1000], Loss: 0.0092\n","Epoch [230/1000], Loss: 0.0088\n","Epoch [240/1000], Loss: 0.0084\n","Epoch [250/1000], Loss: 0.0081\n","Epoch [260/1000], Loss: 0.0078\n","Epoch [270/1000], Loss: 0.0075\n","Epoch [280/1000], Loss: 0.0072\n","Epoch [290/1000], Loss: 0.0070\n","Epoch [300/1000], Loss: 0.0067\n","Epoch [310/1000], Loss: 0.0065\n","Epoch [320/1000], Loss: 0.0063\n","Epoch [330/1000], Loss: 0.0060\n","Epoch [340/1000], Loss: 0.0058\n","Epoch [350/1000], Loss: 0.0057\n","Epoch [360/1000], Loss: 0.0055\n","Epoch [370/1000], Loss: 0.0053\n","Epoch [380/1000], Loss: 0.0051\n","Epoch [390/1000], Loss: 0.0050\n","Epoch [400/1000], Loss: 0.0048\n","Epoch [410/1000], Loss: 0.0047\n","Epoch [420/1000], Loss: 0.0046\n","Epoch [430/1000], Loss: 0.0044\n","Epoch [440/1000], Loss: 0.0043\n","Epoch [450/1000], Loss: 0.0042\n","Epoch [460/1000], Loss: 0.0041\n","Epoch [470/1000], Loss: 0.0040\n","Epoch [480/1000], Loss: 0.0039\n","Epoch [490/1000], Loss: 0.0038\n","Epoch [500/1000], Loss: 0.0037\n","Epoch [510/1000], Loss: 0.0036\n","Epoch [520/1000], Loss: 0.0035\n","Epoch [530/1000], Loss: 0.0034\n","Epoch [540/1000], Loss: 0.0034\n","Epoch [550/1000], Loss: 0.0033\n","Epoch [560/1000], Loss: 0.0032\n","Epoch [570/1000], Loss: 0.0031\n","Epoch [580/1000], Loss: 0.0031\n","Epoch [590/1000], Loss: 0.0030\n","Epoch [600/1000], Loss: 0.0029\n","Epoch [610/1000], Loss: 0.0029\n","Epoch [620/1000], Loss: 0.0028\n","Epoch [630/1000], Loss: 0.0027\n","Epoch [640/1000], Loss: 0.0027\n","Epoch [650/1000], Loss: 0.0026\n","Epoch [660/1000], Loss: 0.0026\n","Epoch [670/1000], Loss: 0.0025\n","Epoch [680/1000], Loss: 0.0025\n","Epoch [690/1000], Loss: 0.0024\n","Epoch [700/1000], Loss: 0.0024\n","Epoch [710/1000], Loss: 0.0023\n","Epoch [720/1000], Loss: 0.0023\n","Epoch [730/1000], Loss: 0.0022\n","Epoch [740/1000], Loss: 0.0022\n","Epoch [750/1000], Loss: 0.0022\n","Epoch [760/1000], Loss: 0.0021\n","Epoch [770/1000], Loss: 0.0021\n","Epoch [780/1000], Loss: 0.0020\n","Epoch [790/1000], Loss: 0.0020\n","Epoch [800/1000], Loss: 0.0020\n","Epoch [810/1000], Loss: 0.0019\n","Epoch [820/1000], Loss: 0.0019\n","Epoch [830/1000], Loss: 0.0019\n","Epoch [840/1000], Loss: 0.0018\n","Epoch [850/1000], Loss: 0.0018\n","Epoch [860/1000], Loss: 0.0018\n","Epoch [870/1000], Loss: 0.0018\n","Epoch [880/1000], Loss: 0.0017\n","Epoch [890/1000], Loss: 0.0017\n","Epoch [900/1000], Loss: 0.0017\n","Epoch [910/1000], Loss: 0.0016\n","Epoch [920/1000], Loss: 0.0016\n","Epoch [930/1000], Loss: 0.0016\n","Epoch [940/1000], Loss: 0.0016\n","Epoch [950/1000], Loss: 0.0015\n","Epoch [960/1000], Loss: 0.0015\n","Epoch [970/1000], Loss: 0.0015\n","Epoch [980/1000], Loss: 0.0015\n","Epoch [990/1000], Loss: 0.0015\n","Epoch [1000/1000], Loss: 0.0014\n","the boy who lived mr and mrs dursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number four\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","# data I/O\n","data = open('harry_potter.txt', 'r').read() \n","raw_dataset = ' '.join(re.sub('[^A-Za-z ]+', '', data).lower().split())\n","data_size = len(raw_dataset)\n","chars = list(set(raw_dataset))\n","vocab_size = len(chars)\n","char_to_ix = { ch:i for i,ch in enumerate(chars) }\n","ix_to_char = { i:ch for i,ch in enumerate(chars) }\n","\n","# hyperparameters\n","hidden_size = 100 \n","seq_length = 50 \n","learning_rate = 1e-1\n","num_layers = 1 \n","\n","# define the model\n","class GRU(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers):\n","        super(GRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, num_layers)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","        \n","    def forward(self, inputs, hidden):\n","        embeds = self.embedding(inputs).view(-1, 1, self.hidden_size)\n","        \n","        output, hidden = self.gru(embeds, hidden)\n","        \n","        output = output.view(-1, self.hidden_size)\n","        output = self.fc(output)\n","        \n","        return output, hidden\n","\n","model = GRU(vocab_size, hidden_size, num_layers).cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","num_epochs = 1000\n","for epoch in range(num_epochs):\n","    h = torch.zeros(num_layers, 1, hidden_size).cuda()\n","    \n","    inputs = [char_to_ix[ch] for ch in raw_dataset[:seq_length]]\n","    targets = [char_to_ix[ch] for ch in raw_dataset[1:seq_length+1]]\n","    inputs = torch.tensor(inputs, dtype=torch.long).cuda()\n","    targets = torch.tensor(targets, dtype=torch.long).cuda()\n","    \n","    loss = 0\n","    for t in range(seq_length):\n","        output, h = model(inputs[t], h)\n","        loss += criterion(output, targets[t].unsqueeze(0))\n","        \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    if (epoch+1) % 10 == 0:\n","        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n","\n","def generate_text(model, start_sentence, num_chars):\n","    \n","    model.eval()\n","\n","    train_chars = set(char_to_ix.keys())\n","    for c in start_sentence:\n","        if c not in train_chars:\n","            raise ValueError(\"Starting sentence contains character '{}' not present in training dataset\".format(c))\n","\n","    h = torch.zeros(num_layers, 1, hidden_size).cuda()\n","    input = torch.tensor([char_to_ix[start_sentence[0]]], dtype=torch.long).cuda()\n","\n","    text = start_sentence\n","    for i in range(num_chars):\n","        output, h = model(input, h)\n","\n","        probs = nn.functional.softmax(output, dim=1).detach().cpu().numpy().squeeze()\n","        next_char_ix = np.random.choice(vocab_size, p=probs)\n","        next_char = ix_to_char[next_char_ix]\n","\n","        text += next_char\n","\n","        input = torch.tensor([next_char_ix], dtype=torch.long).cuda()\n","\n","    return text\n","\n","\n","generated_text = generate_text(model, \"t\", 1000)\n","print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWQQXoKHcX0B","executionInfo":{"status":"ok","timestamp":1678253851256,"user_tz":300,"elapsed":38436,"user":{"displayName":"Jeong Ho Park","userId":"12726191324453319419"}},"outputId":"7ad9d8bd-576e-4b9b-b319-98a733b14040"},"execution_count":143,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/1000], Loss: 1.3431\n","Epoch [20/1000], Loss: 0.0490\n","Epoch [30/1000], Loss: 0.0121\n","Epoch [40/1000], Loss: 0.0048\n","Epoch [50/1000], Loss: 0.0030\n","Epoch [60/1000], Loss: 0.0025\n","Epoch [70/1000], Loss: 0.0022\n","Epoch [80/1000], Loss: 0.0020\n","Epoch [90/1000], Loss: 0.0018\n","Epoch [100/1000], Loss: 0.0017\n","Epoch [110/1000], Loss: 0.0016\n","Epoch [120/1000], Loss: 0.0015\n","Epoch [130/1000], Loss: 0.0014\n","Epoch [140/1000], Loss: 0.0014\n","Epoch [150/1000], Loss: 0.0013\n","Epoch [160/1000], Loss: 0.0012\n","Epoch [170/1000], Loss: 0.0012\n","Epoch [180/1000], Loss: 0.0011\n","Epoch [190/1000], Loss: 0.0011\n","Epoch [200/1000], Loss: 0.0010\n","Epoch [210/1000], Loss: 0.0010\n","Epoch [220/1000], Loss: 0.0010\n","Epoch [230/1000], Loss: 0.0009\n","Epoch [240/1000], Loss: 0.0009\n","Epoch [250/1000], Loss: 0.0009\n","Epoch [260/1000], Loss: 0.0009\n","Epoch [270/1000], Loss: 0.0008\n","Epoch [280/1000], Loss: 0.0008\n","Epoch [290/1000], Loss: 0.0008\n","Epoch [300/1000], Loss: 0.0008\n","Epoch [310/1000], Loss: 0.0007\n","Epoch [320/1000], Loss: 0.0007\n","Epoch [330/1000], Loss: 0.0007\n","Epoch [340/1000], Loss: 0.0007\n","Epoch [350/1000], Loss: 0.0007\n","Epoch [360/1000], Loss: 0.0006\n","Epoch [370/1000], Loss: 0.0006\n","Epoch [380/1000], Loss: 0.0006\n","Epoch [390/1000], Loss: 0.0006\n","Epoch [400/1000], Loss: 0.0006\n","Epoch [410/1000], Loss: 0.0006\n","Epoch [420/1000], Loss: 0.0006\n","Epoch [430/1000], Loss: 0.0005\n","Epoch [440/1000], Loss: 0.0005\n","Epoch [450/1000], Loss: 0.0005\n","Epoch [460/1000], Loss: 0.0005\n","Epoch [470/1000], Loss: 0.0005\n","Epoch [480/1000], Loss: 0.0005\n","Epoch [490/1000], Loss: 0.0005\n","Epoch [500/1000], Loss: 0.0005\n","Epoch [510/1000], Loss: 0.0005\n","Epoch [520/1000], Loss: 0.0005\n","Epoch [530/1000], Loss: 0.0004\n","Epoch [540/1000], Loss: 0.0004\n","Epoch [550/1000], Loss: 0.0004\n","Epoch [560/1000], Loss: 0.0004\n","Epoch [570/1000], Loss: 0.0004\n","Epoch [580/1000], Loss: 0.0004\n","Epoch [590/1000], Loss: 0.0004\n","Epoch [600/1000], Loss: 0.0004\n","Epoch [610/1000], Loss: 0.0004\n","Epoch [620/1000], Loss: 0.0004\n","Epoch [630/1000], Loss: 0.0004\n","Epoch [640/1000], Loss: 0.0004\n","Epoch [650/1000], Loss: 0.0004\n","Epoch [660/1000], Loss: 0.0004\n","Epoch [670/1000], Loss: 0.0003\n","Epoch [680/1000], Loss: 0.0003\n","Epoch [690/1000], Loss: 0.0003\n","Epoch [700/1000], Loss: 0.0003\n","Epoch [710/1000], Loss: 0.0003\n","Epoch [720/1000], Loss: 0.0003\n","Epoch [730/1000], Loss: 0.0003\n","Epoch [740/1000], Loss: 0.0003\n","Epoch [750/1000], Loss: 0.0003\n","Epoch [760/1000], Loss: 0.0003\n","Epoch [770/1000], Loss: 0.0003\n","Epoch [780/1000], Loss: 0.0003\n","Epoch [790/1000], Loss: 0.0003\n","Epoch [800/1000], Loss: 0.0003\n","Epoch [810/1000], Loss: 0.0003\n","Epoch [820/1000], Loss: 0.0003\n","Epoch [830/1000], Loss: 0.0003\n","Epoch [840/1000], Loss: 0.0003\n","Epoch [850/1000], Loss: 0.0003\n","Epoch [860/1000], Loss: 0.0003\n","Epoch [870/1000], Loss: 0.0003\n","Epoch [880/1000], Loss: 0.0003\n","Epoch [890/1000], Loss: 0.0003\n","Epoch [900/1000], Loss: 0.0002\n","Epoch [910/1000], Loss: 0.0002\n","Epoch [920/1000], Loss: 0.0002\n","Epoch [930/1000], Loss: 0.0002\n","Epoch [940/1000], Loss: 0.0002\n","Epoch [950/1000], Loss: 0.0002\n","Epoch [960/1000], Loss: 0.0002\n","Epoch [970/1000], Loss: 0.0002\n","Epoch [980/1000], Loss: 0.0002\n","Epoch [990/1000], Loss: 0.0002\n","Epoch [1000/1000], Loss: 0.0002\n","the boy who lived mr and mrs dursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number foursley of number four\n"]}]},{"cell_type":"markdown","source":["The vanilla RNN model's loss did converge this time with a simpler dataset, and it also had a similar output with the word starting with 't'. Furthermore, the loss for the GRU model with the harry_potter.txt dataset had similar results, but vanilla RNN model had better/stable/converged loss output. "],"metadata":{"id":"-tI0TrYuj_Fg"}},{"cell_type":"code","source":[],"metadata":{"id":"7sJVISq1xYpM","executionInfo":{"status":"aborted","timestamp":1678255336949,"user_tz":300,"elapsed":4,"user":{"displayName":"Jeong Ho Park","userId":"12726191324453319419"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TvWpn15uy8Kr"},"execution_count":null,"outputs":[]}]}